* Kaggle Porto Sugero competition 

** Requirements

*** xgboost

As of 2017/11/16, this needs to be built from source to be able to use tree_method 'hist' option.

*** LightGBM

I had to build this from source to be able to use it on Ubuntu 16.04 with a GPU.

*** Other Python packages

Other Python package requirements are in code/requirements.txt.

** Use

All commands should be performed in code/.

*** Preprocess
Centre and scale data, set up stacking folds and create dummy data for testing.

~python preprocess.py~

Dummy data only:

~python preprocess.py --dummy~

*** Fit models
Models are: xgb, xgbHist, xgbBagged, lgbm, nn, nnBagged, svm, logisticRegression, logisticRegressionBagged, randomForest

Folds are: 0, ..., 4

Can train and produce submission file with:

~python train.py config.yaml MODEL --sub~

Fit hyperparameters.

~python train.py config.yaml MODEL --hyperparams~

Check score under cross-validation:

~python train.py config.yaml MODEL --cv~

Train each model for each fold.

~python train.py config.yaml MODEL --fold 0 1 2 3 4~

In one call to train.py, you can specify --hyperparams, --cv and one of --sub or --fold. Hyperparameter fitting always happens first, then score checking with cross-validation, then fitting for a submission file or to folds.

*** Stack
~python stack.py config.yaml~

