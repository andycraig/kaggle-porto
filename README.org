* Kaggle Porto Seguro competition 
My entry for the [[https://www.kaggle.com/c/porto-seguro-safe-driver-prediction][Kaggle Porto Seguro Safe Driver Prediction competition]], for which I had to predict whether or not an individual would make a car insurance claim.

** Models 
My final entries were:

- A stacked ensemble of bagged gradient boosted trees (xgboost version, and LightGBM version), and
- Bagged gradient boosted trees (xgboost version).

Both performed equally well on the public leaderboard. I tried a number of other models, including a neural network implemented in keras, but none performed well enough to justify adding them to the ensemble.

** Code credits
I used target encoding code by XXX, and [[https://www.kaggle.com/cpmpml/extremely-fast-gini-computation][code to compute the normalised Gini coefficient]] written by Kaggle user [[https://www.kaggle.com/cpmpml][cpmpml]].
   
** Take-aways 
This was an interesting competition because of the very low number of positives (people who made an insurance claim), and the close overlap in feature space between positives and negatives (nicely visualised using t-SNE by XXX), and the large number of categorical variables with high cardinality; I handled these using target encoding /a la/ [[https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf][Daniele Micci-Barreca's paper]].

After the competition ended, I realised I should have been monitoring performance on a validation set while training my models, and that not doing so almost certainly led to over-fitting. Comparing the cross-validated score of an xgboost model without monitoring to the same model with monitoring (code: ~train_with_validation_set.py~), the mean normalised Gini score (the metric for this competition) increased from 0.277 to 0.280. This was better than any of the models I used in the competition, so even without any further tuning or ensembling I probably could have improved my leaderboard score by quite a lot - that's a mistake I won't make again!

Given more time, I would particularly have liked to have worked more on improving the neural network. Several of the top entries used neural network models to great effect, and all used a more sophisticated encoding approach than I was. This was a useful lesson for me and I will be reading up on these. 

Perhaps the biggest lesson from this competition came in reading about the solutions of the top competitors: there were a large range of successful approaches, including neural networks and gradient boosting trees but also, e.g., logistic regression (with lots of feature engineering). Competitors using the same algorithm used quite different approaches too. It was clear that there were a wide variety of methods that could be used to approach this problem, with success strongly depending on (in my opinion) flexibility of models and careful cross-validation. This seems to be consisent with a recent theorem from XXX.
** Requirements

*** xgboost

As of 2017/11/16, this needs to be built from source to be able to use tree_method 'hist' option.

*** LightGBM

I had to build this from source to be able to use it on Ubuntu 16.04 with a GPU.

*** Other Python packages

Other Python package requirements are in ~code/requirements.txt~.

** Use

All commands should be performed in ~code/~.

*** Preprocess
Centre and scale data, set up stacking folds and create dummy data for testing.

~python preprocess.py~

Dummy data only:

~python preprocess.py --dummy~

*** Fit models
Models are: xgb, xgbHist, xgbBagged, lgbm, nn, nnBagged, svm, logisticRegression, logisticRegressionBagged, randomForest

Folds are: 0, ..., 4

Can train and produce submission file with:

~python train.py config.yaml MODEL --sub~

Fit hyperparameters.

~python train.py config.yaml MODEL --hyperparams~

Check score under cross-validation:

~python train.py config.yaml MODEL --cv~

Train each model for each fold, for stacking.

~python train.py config.yaml MODEL --fold 0 1 2 3 4~

Train model on all data, for stacking.

~python train.py config.yaml MODEL --fold -1~

In one call to train.py, you can specify ~--hyperparams~, ~--cv~ and one of ~--sub~ or ~--fold~. Hyperparameter fitting always happens first, then score checking with cross-validation, then fitting for a submission file or to folds.

*** Stack
~python stack.py config.yaml~

