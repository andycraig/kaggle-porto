* TODO Refactor fold training code. :misc:
* DONE Add tuning to LightGBM.                                         :lgbm:
  CLOSED: [2017-11-18 Sat 16:29]
* TODO Find out what best predictive features are. :misc:
* TODO Set up hyperparams for RandomForestClassifier. :randomForest:
* DONE Try RandomForestClassifier.                             :randomForest:
  CLOSED: [2017-11-17 Fri 09:02]
* TODO Try ExtraTreesClassifier. :extraTrees:
* TODO Try linear tree. 
* TODO Tune with min_child_weight allowed to go down to 2. :xgb:
* TODO Add option for hyperparameter tuning by validation set.       :tuning:
Rather than by cross-validation.
For now, just made 2 folds.
* DONE Add option for random (Latin hypercube?) selection of points. :tuning:
  CLOSED: [2017-11-11 Sat 14:12]
Random for now.
* TODO Make selection of parameters log-based. :tuning:
* DONE Refactor out loading of data, and have it done by pickle for speed. :misc:
  CLOSED: [2017-11-15 Wed 18:27]
* DONE Add logistic regression.                                          :lm:
  CLOSED: [2017-11-11 Sat 12:19]
** DONE In train.py.
   CLOSED: [2017-11-11 Sat 12:06]
** DONE In hyperparams.yaml.
   CLOSED: [2017-11-11 Sat 12:19]
* DONE Put layers in hyperparams.yaml.                                   :nn:
  CLOSED: [2017-11-17 Fri 08:54]
* TODO Do something about missing values in train.               :preprocess:
* TODO Set so can train multiple folds in one call of train.py. :misc:
* DONE Add bagged logistic regression.                                   :lm:
  CLOSED: [2017-11-11 Sat 12:19]
** DONE In hyperparams.yaml.
   CLOSED: [2017-11-11 Sat 12:19]
* DONE Remove calc variables.                                    :preprocess:
  CLOSED: [2017-11-11 Sat 12:20]
* DONE Enable jit.                                                     :misc:
  CLOSED: [2017-11-11 Sat 10:24]
Install numba.
* DONE Drop categorical variables?                               :preprocess: 
  CLOSED: [2017-11-11 Sat 12:31]
Compare target-encoded and original versions first.
* DONE Move XGBoost estimator fit_params to parameters to fit().        :xgb:
  CLOSED: [2017-11-17 Fri 08:56]
* DONE Set up xgbHist for hyperparameter training.                      :xgb:
  CLOSED: [2017-11-17 Fri 08:56]
* DONE Set up LightGBM, or set up XGBoost to use 'tree_method':'gpu_hist'. :xgb:
  CLOSED: [2017-11-17 Fri 08:33]
Can't be done with XGBClassifier.
* DONE Better metric for nn. Currently accuracy.                         :nn:
  CLOSED: [2017-11-11 Sat 11:30]
If can't use better metric, consider upsampling positives. Did that.
* DONE Change hyperparams.yaml to use upsample=True.                     :nn:
  CLOSED: [2017-11-11 Sat 12:22]
* DONE Set up nnBagged.                                                  :nn:
  CLOSED: [2017-11-11 Sat 12:32]
* TODO Figure out why early_stopping_rounds=25 causes errors in xgbBagged but not xgb. :xgb:
* TODO Put early_stopping_rounds=25 back in to xgb's in hyperparams.yaml. (Currently breaks StratifiedBaggingClassifier.) :xgb:
* DONE Add target_encode.                                     :preprocessing:
  CLOSED: [2017-11-08 Wed 08:52]
* DONE Handle features with many categories (target_encode).  :preprocessing:
  CLOSED: [2017-11-08 Wed 08:52]
* DONE Implement equivalent of BaggingClassifier, for which subsets can be specified, so can use stratified folds for 'bagged' XGB. :xgb:
  CLOSED: [2017-11-08 Wed 08:31]
* DONE Wrap xgb in StratifiedBaggingClassifier.                         :xgb:
  CLOSED: [2017-11-08 Wed 08:53]
* DONE Make test files. :testconfig: 
    CLOSED: [2017-11-04 Sat 14:47] 
* DONE Move file names into config YAML files.
    CLOSED: [2017-11-04 Sat 14:47]
* DONE Need to stratify stacking on pos/neg?                       :stacking:
  CLOSED: [2017-11-18 Sat 12:34]
* DONE Make application of StandardScaler() a pre-processing step. :scaling:
    CLOSED: [2017-11-04 Sat 14:57]
* TODO Scaler needs to be consistent w.r.t. CV, stacking. :scaling:
* DONE Set up CV for stratifying.                               :hyperparams:
  CLOSED: [2017-11-10 Fri 09:44]
Stratifies automatically.
* DONE To train, add --cv option for estimating accuracy.              :misc:
  CLOSED: [2017-11-09 Thu 18:58]
* TODO Figure out if eval_metric: auc does anything when there is no eval_set specified. :xgb:
* DONE Fix xbgBagged predictions being over 1.                    :xgbBagged:
  CLOSED: [2017-11-09 Thu 18:58]
* DONE Change DNN parameters from testing parameters.                    :nn:
  CLOSED: [2017-11-10 Fri 09:43]
* DONE Set up DNN.                                                       :nn:
  CLOSED: [2017-11-08 Wed 18:35]
* TODO Make hyperparameter tuning report error statistics. :hyperparams:
* TODO Set up xgb.cv with sklearn.StratifiedKFolds. Done, but check what output is. :xgb:
* TODO Scaling may be over fitting :scaling:
